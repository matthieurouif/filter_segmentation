Je vous ai parlé il y a quelques jour de la mesure "Intersection over Union" (ou IoU) qui permet d'évaluer un masque calculé par rapport à la vérité terrain (groundtruth). J'ai ajouté cette mesure à mon code.

L'idée est la suivante. Pour une image donnée, nous avons la segmentation de référence. J'appelle segmentation l'image où la valeur de chaque pixel indique sa classe d'appartenance (nous avons 21 classes dans _Background_). Je calcule avec une méthode donnée (_argmax_ ou _guided filter_) une segmentation candidate. Pour chacune des classes, je déduis un masque pour la segmentation de référence et un masque pour la segmentation candidate. Je peux alors calculer IoU pour cette classe. Puis, je fais la moyenne des IoU sur l'ensemble des classes, ce qui me donne ma moyenne des IoU (a.k.a. mIoU) pour l'image courante. Vous me suivez ?

Voici le problème. Prenons une image qui contient, par exemple, une image de chien. La classe "avion" n'est ni représentée dans la segmentation de référence, ni dans la segmentation candidate (pas de détection d'avion). Les 2 masques étant vides, l'intersection et l'union des masques sont vides, et le ratio intersection / union = 0 / 0 n'est pas défini. Donc faire ? Soit on ignore cette classe pour le calcul de mIoU (on ne considère que les IoU valides), soit on définit IoU = 0 pour cette classe (segmentation candidate complètement incorrect) ou on définit IoU = 1 (segmentation candidate parfaite). Dans la littérature il est proposé de définir IoU = 0. Vous suivez toujours ?

Cette solution n'étant pas satisfaisante, j'ai continué à creuser. Je suis tombé sur une super idée : passer par la matrice de confusion. En effet, la matrice de confusion permet de très simplement calculer l'IoU pour chaque classe (je peux vous expliquer si ça vous intéresse). 

L'idée est d'enrichir la matrice de confusion avec l'intégralité des images de la base de test. Cela revient à coller toutes les images sous forme d'une grosse image et d'évaluer les segmentations sur cette grande image. Pourquoi c'est magique ? La base de donnée contient nécessairement des exemples de chaque classe, ce qui veut dire que dans cette grosse image, aucun des classes n'aura d'union vide ! Cela se traduit par le fait qu'il n'y aura aucune ligne vide dans la matrice de confusion. Une fois la matrice de confusion remplie, le calcule de la mIoU est trivial.

Du coup, j'ai implémenté cette méthode. Ça fonctionne (on dirait), mais c'est assez lent ! Après un peu de recherche, je me suis apercu que 33% du temps de calcul passait dans la mise à jour de matrice de consfusion. Je rappelle qu'ici une segmentation est un tableau ou chaque case (pixel) contient l'index de la classe à laquelle le pixel appartient. Dans un premier temps, je vais `np.flatten()` la segmentation de référence et la segmentation candidate pour simplifier les choses. Le code de mise à jour de la matrice est le suivant :


```
def update_confusion_matrix1(seg0, seg1, matrix):
	for y, x in zip(seg0, seg1):
		matrix[y,x] += 1
```

Pour un pixel donné, si le pixel appartient à la classe 5 dans la segmentation de référence et à la classe 9 dans la segmentation candidate, la matrice de confusion est incrémentée à l'indice (5,9). Sur un exemple synthétique, cette fonction s'exécute en 41 secondes.

Je continue à creuser Internet et je m'apercois que Scikit-learn propose une fonction qui calcule la matrice de confusion. Je la teste donc et celle-ci s'exécute en 42 secondes. Ce n'est pas mieux. Pour info, le code est le suivant :

```
from sklearn.metrics import confusion_matrix

labels = range(21)

def update_confusion_matrix2(seg0, seg1, matrix):
	matrix += confusion_matrix(seg0, seg1, labels)
```

Après pas mal d'essais infructueux, j'ai finalement réussi à faire mieux que ma première version avec un code purement Numpy : 33 secondes. Voici le code :

```
nb_classes = 21
def update_confusion_matrix3(seg0, seg1, matrix):
	for x in np.ravel_multi_index((seg0, seg1), (nb_classes, nb_classes)):
		matrix.flat[x] += 1
```

Ici, j'utilise des indices linéaires pour accéder aux éléments de la matrice de confusion.

Je pense qu'on peut faire mieux que ça, mais ça ira pour mes tests de ce soir. Évidemment, si vous avez une approche encore plus rapide, je prends ! Je peux vous envoyer le code synthétique si vous voulez vous amuser.


Et voici le code de test:

```
from time import time
import numpy as np
from sklearn.metrics import confusion_matrix

# Paramters
size = 500 * 500
nb_classes = 21
labels = range(nb_classes)


def update_confusion_matrix1(seg0, seg1, matrix):
	for y, x in zip(seg0, seg1):
		matrix[y,x] += 1


def update_confusion_matrix2(seg0, seg1, matrix):
	matrix += confusion_matrix(seg0, seg1, labels)


def update_confusion_matrix3(seg0, seg1, matrix):
	for x in np.ravel_multi_index((seg0, seg1), (nb_classes, nb_classes)):
		matrix.flat[x] += 1


# Synthetic segmentations
a = np.random.randint(0, nb_classes, size).flatten()
b = np.random.randint(0, nb_classes, size).flatten()

# Test method 1
matrix = np.zeros((nb_classes, nb_classes), np.int64)
tic = time()
for it in range(10):
	update_confusion_matrix1(a, b, matrix)
toc = time()
print 'Duration 1 : %f' % (toc - tic)

# Test method 2
matrix2 = np.zeros((nb_classes, nb_classes), np.int64)
tic = time()
for it in range(10):
	update_confusion_matrix2(a, b, matrix2)
toc = time()
print 'Duration 2 : %f  --> %s' % (toc - tic, 'SUCCESS' if np.array_equal(matrix, matrix2) else 'FAIL')

# Test method 3
matrix3 = np.zeros((nb_classes, nb_classes), np.int64)
tic = time()
for it in range(10):
	update_confusion_matrix3(a, b, matrix3)
toc = time()
print 'Duration 3 : %f  --> %s' % (toc - tic, 'SUCCESS' if np.array_equal(matrix, matrix3) else 'FAIL')
```